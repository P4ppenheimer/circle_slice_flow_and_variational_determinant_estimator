{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "import pprint\n",
    "import os\n",
    "import time\n",
    "# weights and biases for tracking of metrics\n",
    "import wandb \n",
    "# make the plots inline again\n",
    "%matplotlib inline\n",
    "# sometimes have to activate this to plot plots in notebook\n",
    "# matplotlib.use('Qt5Agg')\n",
    "from code import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pprint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = open(\"config_fit_df.yaml\", 'r')\n",
    "config = yaml.load(stream, Loader = yaml.FullLoader)\n",
    "\n",
    "# This is for the MLPs of the flow\n",
    "config['HIDDEN_DIM_SPLINE_MLP'] = HIDDEN_DIM_SPLINE_MLP\n",
    "config['HIDDEN_DIM_MOEBIUS_MLP'] = HIDDEN_DIM_MOEBIUS_MLP\n",
    "config['HIDDEN_DIM_ROTATION_MLP'] = HIDDEN_DIM_ROTATION_MLP\n",
    "\n",
    "### Set globabl variables and load data\n",
    "BATCH_SIZE = config['BATCH_SIZE']\n",
    "\n",
    "# NUM_FLOWS_COU = config['NUM_FLOWS_COU']\n",
    "# NUM_FLOWS_CYL = config['NUM_FLOWS_CYL']\n",
    "# NUM_CENTERS = config['NUM_CENTERS']\n",
    "# NUM_BINS = config['NUM_BINS']\n",
    "# NUM_DIM_DATA = config['NUM_DIM_DATA']\n",
    "\n",
    "# flow params\n",
    "NUM_DIM_DATA = 256\n",
    "NUM_CENTERS = 12\n",
    "NUM_BINS = 32\n",
    "NUM_FLOWS_COU = 6\n",
    "NUM_FLOWS_CYL = 6\n",
    "\n",
    "CAP_HOUSEHOLDER_REFL = True\n",
    "\n",
    "nr_mixtures = config['nr_mixtures']\n",
    "epochs = config['epochs']\n",
    "eval_iter = config['eval_iter']\n",
    "print_iter = config['print_iter']\n",
    "lookahead = config['lookahead']\n",
    "nr_datapoints = config['nr_datapoints_eval']\n",
    "\n",
    "lr = config['lr']\n",
    "weight_decay = config['weight_decay']\n",
    "\n",
    "# data parameters\n",
    "# nr_mixtures = config['nr_mixtures']\n",
    "\n",
    "# dataset type\n",
    "\n",
    "dataset_type = config['dataset_type']\n",
    "\n",
    "ITERS_PER_EPOCH = config['iters_per_epoch']\n",
    "\n",
    "PROJECT_NAME = f'DIM_{NUM_DIM_DATA}'\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get location and scale parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code for creating dataset on the poles\n",
    "# from random import random\n",
    "# eps = random()*0.2\n",
    "# mu_list = np.array([[eps, eps, 1+eps],\n",
    "#                          [eps, eps, -1+eps],\n",
    "#                          [1, eps, eps]], dtype='float32')\n",
    "\n",
    "# k_list = 1.5*np.array([13, 14,  12],dtype='float32')\n",
    "\n",
    "# mu_list = mu_list / np.linalg.norm(mu_list,axis=1,keepdims=True)\n",
    "# np.linalg.norm(mu_list,axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "if NUM_DIM_DATA == 3 and dataset_type == 'standard':\n",
    "\n",
    "    mu_list = np.array([[-4.7503373e-01, -8.7996745e-01, -5.0922018e-04],\n",
    "                        [-1.6167518e-01,  6.5595394e-01, -7.3728257e-01],\n",
    "                        [ 2.6248896e-01,  6.9851363e-01,  6.6571641e-01],\n",
    "                        [ 1.0,  0,  0.0]], dtype='float32')\n",
    "\n",
    "    k_list = 1.5*np.array([13, 14,  12, 15],dtype='float32')\n",
    "\n",
    "    phi_mu_list = np.array([-2.06579875,  1.81245307,  1.2113401, 0],dtype='float32')\n",
    "    theta_mu_list =  np.array([-5.09220198e-04, -8.29039128e-01,  7.28453441e-01, 0],dtype='float32')\n",
    "    \n",
    "elif NUM_DIM_DATA == 3 and dataset_type == 'poles':\n",
    "    \n",
    "    mu_list = np.array([[ 0.14887695,  0.14887695,  0.9775844 ],\n",
    "                        [ 0.20918883,  0.20918883, -0.9552383 ],\n",
    "                        [ 0.96920884,  0.17411795,  0.17411795]], dtype='float32')\n",
    "    \n",
    "    k_list = 1.5*np.array([13, 14,  12],dtype='float32')\n",
    "    \n",
    "    phi_mu_list = np.array([0.7853982, 0.7853982, 0.1777535],dtype='float32')\n",
    "    theta_mu_list =  np.array([ 1.3586651 , -1.2704642 ,  0.17500998],dtype='float32')\n",
    "    \n",
    "elif NUM_DIM_DATA == 3 and dataset_type == 'poles_2':\n",
    "    \n",
    "    mu_list = np.array([[ 0,  0,  1 ],\n",
    "                        [ 0,  0, -1 ]], dtype='float32')\n",
    "    \n",
    "    k_list = 1.5*np.array([13, 14],dtype='float32')\n",
    "    \n",
    "    phi_mu_list = np.array([0, 0],dtype='float32')\n",
    "    theta_mu_list =  np.array([ np.pi/2 , -np.pi/2],dtype='float32')    \n",
    "    \n",
    "else:    \n",
    "    torch.manual_seed(42)\n",
    "    mu_list, k_list, phi_mu_list, theta_mu_list = create_random_parameters(nr_mixtures, num_dim_data = NUM_DIM_DATA)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d and N dim Power Spherical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Spherical \n",
    "\n",
    "# print(f'Center parameters \\n {mu_list}')\n",
    "# print(f'Scale parameters \\n {k_list}')\n",
    "\n",
    "power_spherical_data = PowerSphericalData(mu_list=mu_list, k_list=k_list, nr_samples=BATCH_SIZE*ITERS_PER_EPOCH)\n",
    "\n",
    "print(f'Entropy {power_spherical_data.entropy.detach().numpy()}')\n",
    "\n",
    "# add parameters to config\n",
    "config['nr_mixtures'] = len(mu_list)\n",
    "config['mu_list'] = mu_list\n",
    "config['k_list'] = k_list\n",
    "config['phi_mu_list'] = phi_mu_list\n",
    "config['theta_mu_list'] = theta_mu_list\n",
    "\n",
    "if NUM_DIM_DATA == 3:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    probs, probs_with_cos, phi_linspace, theta_linspace = plot_power_spherical_density(mu_list, k_list,phi_mu_list,theta_mu_list)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    dphi = phi_linspace[1] - phi_linspace[0]\n",
    "    dtheta = theta_linspace[1] - theta_linspace[0]\n",
    "\n",
    "    print('numerical integral of density w', torch.sum(probs_with_cos) * dphi * dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    \n",
    "    total_number_params = 0\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        total_number_params += parameter.numel()\n",
    "    return total_number_params\n",
    "\n",
    "def train_model(model, \n",
    "                optimizer, \n",
    "                dataset, \n",
    "                config,\n",
    "                model_name):\n",
    "    \n",
    "    \n",
    "    epochs = config['epochs']\n",
    "    config['NUM_PARAMS'] = get_num_params(model)\n",
    "\n",
    "    # init weights and biases tracking\n",
    "    ts = time.strftime('%m%d_%H%M%S', time.localtime(time.time()))\n",
    "    \n",
    "    if model.flow_type == 'moebius':\n",
    "        model_name = f\"{model_name}_NC_{NUM_CENTERS}_NF_{model.num_flows}\"\n",
    "        \n",
    "    elif model.flow_type == 'spline':\n",
    "        model_name = f\"{model_name}_NB_{NUM_BINS}_NF_{model.num_flows}\"\n",
    "        \n",
    "    wandb.init(project=PROJECT_NAME,\n",
    "               config=config,\n",
    "               name=model_name,\n",
    "               id=ts)\n",
    "    \n",
    "    best_KL = 1e3\n",
    "    epoch_of_best_run = 0\n",
    "    \n",
    "    # According to w and b documentation this is magic. Okay, let's see\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    print('##### Model #####')\n",
    "    print(model)\n",
    "    print('#################\\n')\n",
    "    \n",
    "    print('##### Config #####')\n",
    "    pp = pprint.PrettyPrinter(indent=1)\n",
    "    pp.pprint(config)\n",
    "    print('##################\\n')    \n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=config['BATCH_SIZE'], shuffle=True)\n",
    "    \n",
    "    # get properties of dataset class\n",
    "    phi_mu_list, theta_mu_list = dataset.spherical_parameters\n",
    "    entropy = dataset.entropy\n",
    "    \n",
    "    print(f'Entropy of data: {entropy}')\n",
    "\n",
    "    max_steps = config['iters_per_epoch']\n",
    "    \n",
    "    num_dim_data = model.num_dim_data\n",
    "    \n",
    "    # initialize loss. \n",
    "    loss_fn = Loss_on_sphere(n_dim_sphere = num_dim_data - 1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(f\"Epoch: {epoch} / {epochs-1}\")\n",
    "        \n",
    "        if num_dim_data == 3:\n",
    "        \n",
    "            # actually the calculation here is partly redundant because we anyway later evalaute the model on a test set            \n",
    "            eval_and_plot_model(model, \n",
    "                                nr_gridpoints = 100, \n",
    "                                epoch = epoch, \n",
    "                                batch_idx = 0, \n",
    "                                phi_mu_list=phi_mu_list, \n",
    "                                theta_mu_list=theta_mu_list, \n",
    "                                x_conditioner = None)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for batch_idx, x_train in enumerate(train_loader): \n",
    "\n",
    "            x_train = x_train.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z, ldj, _ = model(x_train)\n",
    "\n",
    "            loss = loss_fn.calc_loss(ldj)\n",
    "                          \n",
    "            # calculate gradient with repsect to ldj and not whole nll, \n",
    "            # because our prior is uniform and therefore this way is less expensive\n",
    "            (-torch.mean(ldj)).backward()\n",
    "#             loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            nll = float(loss.detach().cpu().numpy()) \n",
    "            KL = nll - entropy\n",
    "                            \n",
    "            if batch_idx % config['print_iter'] == 0: \n",
    "                print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} with KL {KL:.2f} and nll loss {nll:.2f}')                \n",
    "                  \n",
    "            def eval_model():\n",
    "                \n",
    "                nr_datapoints = config['nr_datapoints_eval']\n",
    "\n",
    "                # KL evaluation based on nr_datapoints samples from data distribution\n",
    "                test_data = dataset.get_test_set(nr_samples = nr_datapoints).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    model.eval()\n",
    "                    z, ldj, _ = model(test_data)\n",
    "\n",
    "                    nll_test = loss_fn.calc_loss(ldj)\n",
    "                    KL_test = nll_test - entropy\n",
    "\n",
    "                    # MC evaluation based on nr_datapoints uniform samples on the sphere\n",
    "                    x_eval = torch.randn(nr_datapoints, num_dim_data).to(device)\n",
    "                    x_eval = x_eval / torch.norm(x_eval, dim=1, keepdim=True)\n",
    "\n",
    "                    z, ldj0, _ = model(x_eval)\n",
    "                    model.train()                \n",
    "\n",
    "                print()\n",
    "                print(f'#### EVALUATION ####')\n",
    "                print(f'Evaluation based on {nr_datapoints} data points from data distribution samples')\n",
    "                print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} KL_test {KL_test} and nll_test {nll_test}')\n",
    "\n",
    "                print('\\nFrom current batch')\n",
    "                print(f'Epoch: {epoch}, i/N: {batch_idx}/{max_steps} avg ldj from current batch {torch.mean(ldj):.2f}')\n",
    "\n",
    "                print(f'\\nFrom {nr_datapoints} uniform samples')\n",
    "                print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} MC density sum {torch.mean(torch.exp(ldj0)):.2f}')\n",
    "                print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} avg ldj {torch.mean(ldj0):.2f}')\n",
    "                print('#####################')\n",
    "                print()\n",
    "                \n",
    "                wandb.log({\"KL_train\": KL,\n",
    "                           \"neg_log_likel_train\": nll,\n",
    "                           \"KL_test\": KL_test,\n",
    "                           \"neg_log_likel_test\": nll_test,            \n",
    "                           \"MC_integral\": torch.mean(torch.exp(ldj0))})  \n",
    "                \n",
    "                return KL_test, nll_test\n",
    "                \n",
    "                \n",
    "            if batch_idx % config['eval_iter'] == 0:\n",
    "                KL_test, nll_test = eval_model()\n",
    "                \n",
    "#                 # This part is for additional evaluation of \n",
    "#                 model.eval()\n",
    "#                 print()\n",
    "#                 print('#####################')\n",
    "\n",
    "#                 # first, second and both coupling w/o rotation\n",
    "#                 sldj_test = torch.zeros(nr_datapoints).to(device)       \n",
    "                \n",
    "#                 x_out, ldj_first, _ = model.scale[1](x_eval, sldj_test) \n",
    "#                 _, ldj_second, _  = model.scale[3](x_eval, sldj_test) \n",
    "                \n",
    "#                 _, ldj_first_second, _  = model.scale[3](x_out, ldj_first)                  \n",
    "\n",
    "#                 print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} MC density sum first flow only moeb {torch.mean(torch.exp(ldj_first)):.2f}')\n",
    "#                 print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} MC density sum second flow only moeb {torch.mean(torch.exp(ldj_second)):.2f}') \n",
    "#                 print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} MC density sum first and second flow only moeb {torch.mean(torch.exp(ldj_first_second)):.2f}')\n",
    "#                 print()\n",
    "                \n",
    "#                 sldj_test = torch.zeros(nr_datapoints).to(device)\n",
    "#                 x_out, ldj, _ = model.scale[0](x_eval, sldj_test)\n",
    "#                 x_out, ldj1, _ = model.scale[1](x_out, ldj)\n",
    "                \n",
    "#                 print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} MC density first flow incl rot {torch.mean(torch.exp(ldj1)):.2f}') \n",
    "                \n",
    "#                 sldj_test = torch.zeros(nr_datapoints).to(device)\n",
    "#                 x_out, ldj, _ = model.scale[2](x_eval, sldj_test)\n",
    "#                 x_out, ldj2, _ = model.scale[3](x_out, ldj)\n",
    "                \n",
    "#                 print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} MC density second flow incl rot {torch.mean(torch.exp(ldj2)):.2f}')   \n",
    "                \n",
    "#                 sldj_test = torch.zeros(nr_datapoints).to(device)\n",
    "#                 x_out, ldj, _ = model.scale[0](x_eval, sldj_test)\n",
    "#                 x_out, ldj, _ = model.scale[1](x_out, ldj)                \n",
    "#                 x_out, ldj, _ = model.scale[2](x_out, ldj)\n",
    "#                 x_out, ldj12, _ = model.scale[3](x_out, ldj)\n",
    "\n",
    "#                 print(f'Epoch: {epoch}/{epochs-1}, i/N: {batch_idx}/{max_steps} MC density first&second flow incl rot {torch.mean(torch.exp(ldj12)):.2f}')                   \n",
    "                \n",
    "#                 print('#####################')\n",
    "#                 print()\n",
    "#                 model.train()                \n",
    "                \n",
    "                # store the metrics in the log. Call logger after eval and plot model because we also log plots inside\n",
    "                   \n",
    "            def store_model(best_vs_last):\n",
    "                if not os.path.exists(f'models_fit_df/dim_{num_dim_data}'):\n",
    "                    os.makedirs(f'models_fit_df/dim_{num_dim_data}')\n",
    "                # also store model in the end\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'batch_index': batch_idx,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'config': config\n",
    "                }                        \n",
    "\n",
    "                torch.save(state, f\"models_fit_df/dim_{num_dim_data}/{best_vs_last}_{model_name}_model.t7\")\n",
    "                wandb.save(f\"models_fit_df/dim_{num_dim_data}/{best_vs_last}_{model_name}_model.t7\")  \n",
    "                \n",
    "\n",
    "            # save the best model but do not save any model during the first 50 iterations\n",
    "            if not (epoch == 0 and batch_idx < 50):\n",
    "\n",
    "                if KL_test < best_KL:\n",
    "\n",
    "                    store_model(best_vs_last='best')\n",
    "\n",
    "                    best_KL = KL_test\n",
    "                    epoch_of_best_run = epoch\n",
    "                    \n",
    "                    wandb.run.summary[\"KL_best\"] = KL_test\n",
    "                    wandb.run.summary[\"nll_best\"] = nll_test\n",
    "                    \n",
    "                    wandb.run.summary[\"epoch_of_best_KL\"] = epoch_of_best_run\n",
    "                    wandb.run.summary[\"batch_id_of_best_KL\"] = batch_idx                    \n",
    "                    \n",
    "                    wandb.run.summary[\"KL_current_batch\"] = KL\n",
    "                    wandb.run.summary[\"nll_current_batch\"] = nll\n",
    "           \n",
    "        \n",
    "        wandb.log({\"time_per_epoch\": time.time()-start_time},commit=False)\n",
    "        \n",
    "        # if there was no improvement after a certain amount of epochs terminate training    \n",
    "        if (epoch - epoch_of_best_run) >= config['lookahead']:\n",
    "            print()\n",
    "            print('#### EARLY STOPPING ####')\n",
    "            print(f'at epoch {epoch} and batch_id {batch_idx}')\n",
    "            print('########################')\n",
    "            print()\n",
    "            break\n",
    "                \n",
    "                                        \n",
    "    # also store model in the end        \n",
    "    store_model(best_vs_last='last')\n",
    "    \n",
    "    wandb.run.summary[\"last_epoch\"] = epoch\n",
    "    \n",
    "    _,_ = eval_model()\n",
    "    \n",
    "    if num_dim_data == 3:\n",
    "        eval_and_plot_model(model, \n",
    "                            nr_gridpoints = 100, \n",
    "                            epoch = epoch, \n",
    "                            batch_idx = 0, \n",
    "                            phi_mu_list=phi_mu_list, \n",
    "                            theta_mu_list=theta_mu_list, \n",
    "                            x_conditioner = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Moebius Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cou_moeb = Coupling_Flow(num_flows = NUM_FLOWS_COU, \n",
    "                              num_dim_data= NUM_DIM_DATA,\n",
    "                              flow_type = 'moebius', \n",
    "                              rezero_flag = True,\n",
    "                              num_centers = NUM_CENTERS,\n",
    "                              cap_householder_refl=CAP_HOUSEHOLDER_REFL)\n",
    "\n",
    "cou_moeb.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(cou_moeb.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('##### Coupling Moebius Flow #####')\n",
    "\n",
    "\n",
    "train_model(model = cou_moeb, \n",
    "            optimizer = optimizer,\n",
    "            dataset = power_spherical_data,\n",
    "            config = config,\n",
    "            model_name='cou_m')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Spline Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cou_spline = Coupling_Flow(num_flows = NUM_FLOWS_COU, \n",
    "                           num_dim_data= NUM_DIM_DATA,\n",
    "                           flow_type = 'spline', \n",
    "                           num_centers = NUM_CENTERS,\n",
    "                           num_bins = NUM_BINS,\n",
    "                           cap_householder_refl=CAP_HOUSEHOLDER_REFL) \n",
    "\n",
    "cou_spline.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(cou_spline.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('##### Coupling Spline Flow #####')\n",
    "\n",
    "train_model(model = cou_spline, \n",
    "            optimizer = optimizer,\n",
    "            dataset = power_spherical_data,\n",
    "            config = config,\n",
    "            model_name = 'cou_s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cylindrical Moebius flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyl_moeb = Cylindrical_Flow(num_flows=NUM_FLOWS_CYL,\n",
    "                                 num_bins=NUM_BINS, \n",
    "                                 flow_type='moebius',\n",
    "                                 num_dim_data=NUM_DIM_DATA, \n",
    "                                 num_centers=NUM_CENTERS)\n",
    "\n",
    "cyl_moeb.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(cyl_moeb.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('##### Cylindrical Moebius Flow #####')\n",
    "\n",
    "train_model(model = cyl_moeb, \n",
    "            optimizer = optimizer,\n",
    "            dataset = power_spherical_data,\n",
    "            config = config,\n",
    "            model_name='ar_cyl_m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cylindrical Spline flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyl_spline = Cylindrical_Flow(num_flows=NUM_FLOWS_CYL,\n",
    "                                 num_bins=NUM_BINS,\n",
    "                                 flow_type= 'spline',\n",
    "                                 num_dim_data = NUM_DIM_DATA)\n",
    "\n",
    "cyl_spline.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(cyl_spline.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('##### Cylindrical Spline Flow #####')\n",
    "\n",
    "train_model(model = cyl_spline, \n",
    "            optimizer = optimizer,\n",
    "            dataset = power_spherical_data,\n",
    "            config = config,\n",
    "            model_name='ar_cyl_s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Cylindrical Moebius flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyl_moeb = Cylindrical_Flow(num_flows=NUM_FLOWS_COU,\n",
    "                                 num_bins=NUM_BINS, \n",
    "                                 flow_type='moebius',\n",
    "                                 num_dim_data=NUM_DIM_DATA, \n",
    "                                 mask_type='coupling',\n",
    "                                 num_centers=NUM_CENTERS)\n",
    "\n",
    "cyl_moeb.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(cyl_moeb.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('##### Coupling Cylindrical Moebius Flow #####')\n",
    "\n",
    "train_model(model = cyl_moeb, \n",
    "            optimizer = optimizer,\n",
    "            dataset = power_spherical_data,\n",
    "            config = config,\n",
    "            model_name='cou_cyl_m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Cylindrical Spline flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyl_spline = Cylindrical_Flow(num_flows=NUM_FLOWS_COU,\n",
    "                                 num_bins=NUM_BINS, \n",
    "                                 flow_type='spline',\n",
    "                                 num_dim_data=NUM_DIM_DATA, \n",
    "                                 mask_type='coupling',\n",
    "                                 num_centers=NUM_CENTERS)\n",
    "\n",
    "cyl_spline.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(cyl_spline.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('##### Coupling Cylindrical Spline Flow #####')\n",
    "\n",
    "train_model(model = cyl_spline, \n",
    "            optimizer = optimizer,\n",
    "            dataset = power_spherical_data,\n",
    "            config = config,\n",
    "            model_name='cou_cyl_s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow_lab",
   "language": "python",
   "name": "flow_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
